{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "fZWSMkFLhXU_",
        "outputId": "231dcdda-1a79-4c18-f55c-cb88f60b58e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample business_id from final_reviews: [\" 'strange'\", nan, nan, nan, nan]\n",
            "Sample business_id from business_final: ['MTSW4McQd7CbVtyjqoe9mw', 'MUTTqe8uqyMdBl186RmNeA', 'ROeacJQwBeh05Rqg7F6TCg', 'aPNXGTDkf-4bjhyMBQxqpQ', 'ppFCk9aQkM338Rgwpl2F5A']\n",
            "Unique business_id in final_reviews: 3878\n",
            "Unique business_id in business_final: 3912\n",
            "Error: Merged DataFrame is empty. No matching business_ids found.\n",
            "Common business_ids: set()\n",
            "If set is empty, check data integrity in final_reviews.csv and business_final.csv.\n",
            "super_score computed successfully.\n",
            "Error: 'cleaned_text' column missing or all NaN.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'maxlen' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-33c54947064c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cnn_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mX_text_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Specify batch_size to avoid issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-33c54947064c>\u001b[0m in \u001b[0;36mbuild_cnn_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_cnn_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     model = models.Sequential([\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'maxlen' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Read the CSV files with error handling\n",
        "try:\n",
        "    final_reviews = pd.read_csv(\"/content/final_reviews.csv\", quoting=3, on_bad_lines='skip')\n",
        "    business_final = pd.read_csv(\"/content/business_final.csv\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Debug: Inspect and clean business_id\n",
        "print(\"Sample business_id from final_reviews:\", final_reviews['business_id'].head().tolist())\n",
        "print(\"Sample business_id from business_final:\", business_final['business_id'].head().tolist())\n",
        "print(\"Unique business_id in final_reviews:\", final_reviews['business_id'].nunique())\n",
        "print(\"Unique business_id in business_final:\", business_final['business_id'].nunique())\n",
        "\n",
        "# Clean business_id: Convert to string, strip whitespace, and handle NaN\n",
        "final_reviews['business_id'] = final_reviews['business_id'].astype(str).str.strip()\n",
        "business_final['business_id'] = business_final['business_id'].astype(str).str.strip()\n",
        "final_reviews = final_reviews[final_reviews['business_id'].notna() & (final_reviews['business_id'] != 'nan') & (final_reviews['business_id'] != \"'strange'\")]\n",
        "\n",
        "# Merge DataFrames\n",
        "df = pd.merge(final_reviews, business_final, on='business_id', how=\"inner\")\n",
        "if df.empty:\n",
        "    common_ids = set(final_reviews['business_id']).intersection(set(business_final['business_id']))\n",
        "    print(\"Error: Merged DataFrame is empty. No matching business_ids found.\")\n",
        "    print(\"Common business_ids:\", common_ids)\n",
        "    print(\"If set is empty, check data integrity in final_reviews.csv and business_final.csv.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"Merged DataFrame shape:\", df.shape)\n",
        "    print(df.head())\n",
        "\n",
        "# Compute super_score\n",
        "required_cols = ['stars_x', 'polarity', 'compound']\n",
        "if all(col in df.columns for col in required_cols):\n",
        "    df['super_score'] = df['stars_x'] + (df['polarity'] * df['compound'])\n",
        "    print(\"super_score computed successfully.\")\n",
        "else:\n",
        "    print(\"Error: Missing columns for super_score:\", [col for col in required_cols if col not in df.columns])\n",
        "    exit()\n",
        "\n",
        "# Discretize super_score into categories\n",
        "bins = [0, 3, 4.5, 6]  # Adjust as needed\n",
        "labels = ['Low', 'Medium', 'High']\n",
        "df['demand_category'] = pd.cut(df['super_score'], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "# Step 1: Text preprocessing for CNN\n",
        "if 'cleaned_text' in df.columns and not df['cleaned_text'].isna().all():\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(df['cleaned_text'].fillna(''))  # Fill NaN with empty string\n",
        "    sequences = tokenizer.texts_to_sequences(df['cleaned_text'].fillna(''))\n",
        "    maxlen = 50\n",
        "    X_text = pad_sequences(sequences, maxlen=maxlen)\n",
        "    if X_text.size == 0:\n",
        "        print(\"Error: X_text is empty after preprocessing. Check cleaned_text data.\")\n",
        "        exit()\n",
        "else:\n",
        "    print(\"Error: 'cleaned_text' column missing or all NaN.\")\n",
        "    exit()\n",
        "\n",
        "# Step 2: Build CNN for text feature extraction\n",
        "def build_cnn_text():\n",
        "    model = models.Sequential([\n",
        "        layers.Embedding(input_dim=5000, output_dim=64, input_length=maxlen),\n",
        "        layers.Conv1D(128, 5, activation='relu'),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 5, activation='relu'),\n",
        "        layers.GlobalMaxPooling1D(),\n",
        "        layers.Dense(32, activation='relu')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "cnn = build_cnn_text()\n",
        "X_text_features = cnn.predict(X_text, batch_size=32)  # Specify batch_size to avoid issues\n",
        "\n",
        "# Step 3: Combine text features with numerical features\n",
        "required_cols = ['super_score', 'latitude', 'longitude', 'review_count']\n",
        "if all(col in df.columns for col in required_cols):\n",
        "    X_num = df[required_cols].values\n",
        "    X_combined = np.hstack((X_text_features, X_num))\n",
        "else:\n",
        "    print(f\"Error: Missing columns: {[col for col in required_cols if col not in df.columns]}\")\n",
        "    exit()\n",
        "\n",
        "# Step 4: Prepare target and split data\n",
        "y = df['demand_category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train classifiers\n",
        "classifiers = {\n",
        "    \"SVM\": SVC(kernel='rbf', probability=True),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"RF\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[name] = {\"predictions\": y_pred, \"accuracy\": accuracy}\n",
        "\n",
        "# Step 6: Generate business insights\n",
        "def generate_business_insights(preds, classifier_name, accuracy, test_data):\n",
        "    pred_df = pd.DataFrame({'business_id': df.iloc[test_data.index]['business_id'], 'prediction': preds})\n",
        "    demand_dist = pred_df['prediction'].value_counts(normalize=True) * 100\n",
        "    high_pct = demand_dist.get('High', 0.0)\n",
        "    low_pct = demand_dist.get('Low', 0.0)\n",
        "    insights = f\"Using {classifier_name} (Accuracy: {accuracy:.2f}): {high_pct:.2f}% of restaurants predicted as High demand.\"\n",
        "    recommendation = (\n",
        "        f\"Focus on promoting High demand restaurants to attract more customers. \"\n",
        "        f\"Low demand ({low_pct:.2f}%) may need menu updates or marketing boosts.\"\n",
        "    )\n",
        "    top_high = pred_df[pred_df['prediction'] == 'High']['business_id'].head(5).tolist()\n",
        "    return insights, recommendation, top_high\n",
        "\n",
        "# Output results\n",
        "for name, result in results.items():\n",
        "    insights, rec, top_high = generate_business_insights(result[\"predictions\"], name, result[\"accuracy\"], X_test)\n",
        "    print(f\"\\nClassifier: {name}\")\n",
        "    print(\"Insights:\", insights)\n",
        "    print(\"Recommendation:\", rec)\n",
        "    print(\"Top 5 High Demand Restaurants:\", top_high)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZKVKjXFar3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3_zAtPKjKtN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}